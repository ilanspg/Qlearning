{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"q_learning_class_example.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"fbGOd7yzt0lg","colab_type":"text"},"source":["![alt text](https://visualstudiomagazine.com/articles/2018/10/18/~/media/ECG/visualstudiomagazine/Images/2018/10/McCaffreyFig1.asxh)"]},{"cell_type":"code","metadata":{"id":"ipUVYj2D5Ziu","colab_type":"code","colab":{}},"source":["import numpy as np"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C9JBs7lItlVe","colab_type":"text"},"source":["The demo program defines a helper method my_print() to display a computed Q matrix:\n","\n","Function my_print() is a hard-coded hack designed to work only with the demo maze."]},{"cell_type":"code","metadata":{"id":"WEexKn8s5po5","colab_type":"code","colab":{}},"source":["def my_print(Q):\n","  # hard-coded hack for this problem only\n","  rows = len(Q); cols = len(Q[0])\n","  print(\"       0      1      2      3      4      5\\\n","      6      7      8      9      10     11     12\\\n","     13     14\")\n","  for i in range(rows):\n","    print(\"%d \" % i, end=\"\")\n","    if i < 10: print(\" \", end=\"\")\n","    for j in range(cols): print(\" %6.2f\" % Q[i,j], end=\"\")\n","    print(\"\")\n","  print(\"\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6UrVUTb5s-B6","colab_type":"text"},"source":["Function get_poss_next_states() is defined:\n","\n","For a given cell state s, the function uses the F matrix to determine which states can be reached, and returns those states as a list. For example, if s = 5, the return list is [0, 6, 10]."]},{"cell_type":"code","metadata":{"id":"yknE1zIm5vZF","colab_type":"code","colab":{}},"source":["def get_poss_next_states(s, F, ns):\n","  # given a state s and a feasibility matrix F\n","  # get list of possible next states\n","  poss_next_states = []\n","  for j in range(ns):\n","    if F[s,j] == 1: poss_next_states.append(j)\n","  return poss_next_states"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y7qEwNpftRy-","colab_type":"text"},"source":["Function get_rnd_next_state() is defined:\n","\n","For a given state s, all possible next states are determined, and then one of those states is randomly selected. For example, if s = 5, the candidates are [0, 6, 10]. The call to np.randomint() returns a random value from 0 to 2, which acts as the index into the candidate list."]},{"cell_type":"code","metadata":{"id":"96ANRJ5k5yG2","colab_type":"code","colab":{}},"source":["\n","def get_rnd_next_state(s, F, ns):\n","  # given a state s, pick a feasible next state\n","  poss_next_states = get_poss_next_states(s, F, ns)\n","  next_state = \\\n","    poss_next_states[np.random.randint(0,\\\n","    len(poss_next_states))]\n","  return next_state "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EQ5remdssMPo","colab_type":"text"},"source":["Most of the work is done by the train() function, which computes the values of the Q matrix. Q-learning is iterative, so a maximum number of iterations, 1,000, is set. Q-learning has two parameters, the learning rate and gamma. Larger values of the learning rate increase the influence of both current rewards and future rewards (explore) at the expense of past rewards (exploit). The value of gamma, also called the discount factor, influences the importance of future rewards. These values must be determined by trial and error but using 0.5 is typically a good starting place.\n","\n","The train() function uses two helpers, get_poss_next_states() and get_rnd_next_state().\n","\n"]},{"cell_type":"code","metadata":{"id":"d6V-FrgH50dP","colab_type":"code","colab":{}},"source":["def train(F, R, Q, gamma, lrn_rate, goal, ns, max_epochs):\n","  # compute the Q matrix\n","  for i in range(0,max_epochs):\n","    curr_s = np.random.randint(0,ns)  # random start state\n","\n","    while(True):\n","      next_s = get_rnd_next_state(curr_s, F, ns)\n","      poss_next_next_states = \\\n","        get_poss_next_states(next_s, F, ns)\n","\n","      max_Q = -9999.99\n","      for j in range(len(poss_next_next_states)):\n","        nn_s = poss_next_next_states[j]\n","        q = Q[next_s,nn_s]\n","        if q > max_Q:\n","          max_Q = q\n","      # Q = [(1-a) * Q]  +  [a * (rt + (g * maxQ))]\n","      Q[curr_s][next_s] = ((1 - lrn_rate) * Q[curr_s] \\\n","        [next_s]) + (lrn_rate * (R[curr_s][next_s] + \\\n","        (gamma * max_Q)))\n","\n","      curr_s = next_s\n","      if curr_s == goal: break"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lMiGu70Qu0uZ","colab_type":"text"},"source":["After the Q matrix has been computed, it can be used by function walk() to show the shortest path from any starting cell to any goal cell in the maze.\n","\n","The key to function walk() is function np.argmax(), which returns the index of the largest value of the function's input vector. For example, if a vector v had values (8, 6, 9, 4, 5, 7), then a call to np.argmax(v) would return 2. Function walk() assumes the goal state is reachable."]},{"cell_type":"code","metadata":{"id":"LnbNPE9156Dg","colab_type":"code","colab":{}},"source":["def walk(start, goal, Q):\n","  # go to goal from start using Q\n","  curr = start\n","  print(str(curr) + \"->\", end=\"\")\n","  while curr != goal:\n","    next = np.argmax(Q[curr])\n","    print(str(next) + \"->\", end=\"\")\n","    curr = next\n","  print(\"done\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NtzoBbVhr0TL","colab_type":"text"},"source":["The main() function begins by setting up an F matrix which defines the feasibility of moving from one cell/state to another. For example, F[7][12] = 1 means you can transition from cell/state 7 to cell/state 12, and F[6][7] = 0 means you cannot move from cell 7 to cell 8 (because there's a wall in the way).\n","\n","The R matrix defines a reward for moving from one cell to another. Most feasible moves give a negative reward of -0.1, which punishes moves that don't make progress and therefore discourages going in circles. The one big reward is R[9][14] = 10.0, which moves you to the goal state."]},{"cell_type":"code","metadata":{"id":"lXHBd1m45-jj","colab_type":"code","colab":{}},"source":["def main():\n","  np.random.seed(1)\n","  print(\"\\nSetting up maze in memory\")\n","\n","  F = np.zeros(shape=[15,15], dtype=np.int)  # Feasible \n","  F[0,1] = 1; F[0,5] = 1; F[1,0] = 1; F[2,3] = 1; F[3,2] = 1\n","  F[3,4] = 1; F[3,8] = 1; F[4,3] = 1; F[4,9] = 1; F[5,0] = 1\n","  F[5,6] = 1; F[5,10] = 1; F[6,5] = 1; F[7,8] = 1; F[7,12] = 1\n","  F[8,3] = 1; F[8,7] = 1; F[9,4] = 1; F[9,14] = 1; F[10,5] = 1\n","  F[10,11] = 1; F[11,10] = 1; F[11,12] = 1; F[12,7] = 1;\n","  F[12,11] = 1; F[12,13] = 1; F[13,12] = 1; F[14,14] = 1\n","\n","  R = np.zeros(shape=[15,15], dtype=np.int)  # Rewards\n","  R[0,1] = -0.1; R[0,5] = -0.1; R[1,0] = -0.1; R[2,3] = -0.1\n","  R[3,2] = -0.1; R[3,4] = -0.1; R[3,8] = -0.1; R[4,3] = -0.1\n","  R[4,9] = -0.1; R[5,0] = -0.1; R[5,6] = -0.1; R[5,10] = -0.1\n","  R[6,5] = -0.1; R[7,8] = -0.1; R[7,12] = -0.1; R[8,3] = -0.1\n","  R[8,7] = -0.1; R[9,4] = -0.1; R[9,14] = 10.0; R[10,5] = -0.1\n","  R[10,11] = -0.1; R[11,10] = -0.1; R[11,12] = -0.1\n","  R[12,7] = -0.1; R[12,11] = -0.1; R[12,13] = -0.1\n","  R[13,12] = -0.1; R[14,14] = -0.1\n","  \n","\n","  Q = np.zeros(shape=[15,15], dtype=np.float32)  # Quality\n","  \n","  print(\"Analyzing maze with RL Q-learning\")\n","  start = 0; goal = 14\n","  ns = 15  # number of states\n","  gamma = 0.5\n","  lrn_rate = 0.5\n","  max_epochs = 1000\n","  train(F, R, Q, gamma, lrn_rate, goal, ns, max_epochs)\n","  print(\"Done \")\n","  \n","  print(\"The Q matrix is: \\n \")\n","  my_print(Q)\n","\n","  print(\"Using Q to go from 0 to goal (14)\")\n","\n","  walk(start, goal, Q)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QBKftUxb6CaY","colab_type":"code","outputId":"0874b665-deac-483b-d0b6-08a195066653","executionInfo":{"status":"ok","timestamp":1578035469564,"user_tz":-120,"elapsed":4434,"user":{"displayName":"Ilan Spiegel","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCLpYt2trdavj2LiPmDUhnNTloJjD30ZfL2a6dqYg=s64","userId":"02081136513523452152"}},"colab":{"base_uri":"https://localhost:8080/","height":442}},"source":["if __name__ == \"__main__\":\n","  main()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\n","Setting up maze in memory\n","Analyzing maze with RL Q-learning\n","Done \n","The Q matrix is: \n"," \n","       0      1      2      3      4      5      6      7      8      9      10     11     12     13     14\n","0     0.00   0.00   0.00   0.00   0.00   0.02   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n","1     0.01   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n","2     0.00   0.00   0.00   1.25   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n","3     0.00   0.00   0.62   0.00   2.50   0.00   0.00   0.00   0.62   0.00   0.00   0.00   0.00   0.00   0.00\n","4     0.00   0.00   0.00   1.25   0.00   0.00   0.00   0.00   0.00   5.00   0.00   0.00   0.00   0.00   0.00\n","5     0.01   0.00   0.00   0.00   0.00   0.00   0.01   0.00   0.00   0.00   0.04   0.00   0.00   0.00   0.00\n","6     0.00   0.00   0.00   0.00   0.00   0.02   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n","7     0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.62   0.00   0.00   0.00   0.16   0.00   0.00\n","8     0.00   0.00   0.00   1.25   0.00   0.00   0.00   0.31   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n","9     0.00   0.00   0.00   0.00   2.50   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  10.00\n","10    0.00   0.00   0.00   0.00   0.00   0.02   0.00   0.00   0.00   0.00   0.00   0.08   0.00   0.00   0.00\n","11    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.04   0.00   0.16   0.00   0.00\n","12    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.31   0.00   0.00   0.00   0.08   0.00   0.08   0.00\n","13    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.16   0.00   0.00\n","14    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n","\n","Using Q to go from 0 to goal (14)\n","0->5->10->11->12->7->8->3->4->9->14->done\n"],"name":"stdout"}]}]}